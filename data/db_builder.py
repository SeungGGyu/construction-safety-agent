

import os
import json
import pickle
from tqdm import tqdm
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.schema import Document

# ===== 1ï¸âƒ£ ê²½ë¡œ ì„¤ì • =====
base_dir = "/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent"
chunk_dir = os.path.join(base_dir, "/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/chunks")  # âœ… chunk íŒŒì¼ ì €ì¥ í´ë”
save_dir = os.path.join(base_dir, "/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/DB")  # âœ… ì €ì¥ ê²½ë¡œ

# ===== 2ï¸âƒ£ Qwen Embedding API ì„¤ì • =====
embedder_model_name = "Qwen/Qwen3-Embedding-4B"
embedder_base_url = "http://211.47.56.71:15653/v1"
embedder_api_key = "token-abc123"       

embedding_model = OpenAIEmbeddings(
    model=embedder_model_name,
    base_url=embedder_base_url,
    api_key=embedder_api_key
)

# ===== 3ï¸âƒ£ chunk íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° =====
chunk_json_path = os.path.join(chunk_dir, "chunks.json")
chunk_pkl_path = os.path.join(chunk_dir, "chunks.pkl")

chunks = []

if os.path.exists(chunk_json_path):
    print(f"ğŸ“‚ JSON íŒŒì¼ì—ì„œ ì²­í¬ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘: {chunk_json_path}")
    with open(chunk_json_path, "r", encoding="utf-8") as f:
        chunks = json.load(f)

elif os.path.exists(chunk_pkl_path):
    print(f"ğŸ“‚ PKL íŒŒì¼ì—ì„œ ì²­í¬ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘: {chunk_pkl_path}")
    with open(chunk_pkl_path, "rb") as f:
        chunks = pickle.load(f)
else:
    raise FileNotFoundError("âŒ chunk.json ë˜ëŠ” chunk.pkl íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

print(f"âœ… ì´ ë¡œë“œëœ ì²­í¬ ìˆ˜: {len(chunks)}")

# ===== 4ï¸âƒ£ LangChain ë¬¸ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ =====
docs = []
for c in tqdm(chunks, desc="ğŸ§© Document ë³€í™˜ ì¤‘"):
    # chunk êµ¬ì¡° ì˜ˆì‹œ:
    # {"content": "...", "source": "file.md", "section": "ì œ1ì¥ ì´ì¹™"}
    text = c.get("content") or c.get("page_content") or ""
    meta = {
        "source": c.get("source", "unknown"),
        "section": c.get("section", c.get("heading", "ë³¸ë¬¸"))
    }
    docs.append(Document(page_content=text, metadata=meta))

print(f"âœ… LangChain ë¬¸ì„œ ê°œì²´ë¡œ ë³€í™˜ ì™„ë£Œ: {len(docs)}ê°œ")

# ===== 5ï¸âƒ£ FAISS ë²¡í„° DB ìƒì„± =====
print("ğŸ§  ì„ë² ë”© ë° ë²¡í„° ìƒì„± ì¤‘... (Qwen3-Embedding-4B)")
db = FAISS.from_documents(docs, embedding_model)

# ===== 6ï¸âƒ£ DB ì €ì¥ =====
os.makedirs(save_dir, exist_ok=True)
db.save_local(save_dir)

print(f"\nâœ… RAG ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ!")
print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {save_dir}")
print(f"ğŸ§© ì´ ì²­í¬ ìˆ˜: {len(docs)}")

# ===== 7ï¸âƒ£ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ =====
query = "ì´ë™ì‹ì‘ì—…ëŒ€ì°¨ ì•ˆì „ì¡°ì¹˜"
results = db.similarity_search(query, k=3)

print("\nğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼:")
for r in results:
    print(f"\n[íŒŒì¼] {r.metadata.get('source', '')} | [ì„¹ì…˜] {r.metadata.get('section', '')}")
    print(r.page_content[:300], "...\n")
